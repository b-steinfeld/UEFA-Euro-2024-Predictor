{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "OOEhPBj_a-gc"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import time\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialze a database that we will add to.\n",
        "all_matches = []\n",
        "\n",
        "# Starting URL for webscrapping.\n",
        "euros_url = \"https://fbref.com/en/comps/676/European-Championship-Stats\""
      ],
      "metadata": {
        "id": "UvKjpZxPbLEX"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Group A - Group F (therefore, 6 groups to go through.)\n",
        "for i in range(6):\n",
        "  # Grab table.\n",
        "  data = requests.get(euros_url)\n",
        "  soup = BeautifulSoup(data.text)\n",
        "  # This is a CSS selector; selects any table element that has the tag 'table' and class 'stats_table'.\n",
        "  euros_table = soup.select('table.stats_table')[i]\n",
        "\n",
        "  # Grab each team from the table. \n",
        "  # The link we need has an 'a' tag with an href and must have '/squads' in the link. \n",
        "  links = [l.get(\"href\") for l in euros_table.find_all('a')]\n",
        "  links = [l for l in links if '/squads' in l]\n",
        "  team_urls = [f\"https://fbref.com{l}\" for l in links]\n",
        "\n",
        "  # Prevent site from blocking requests.\n",
        "  time.sleep(20)\n",
        "\n",
        "  # Go through each team.\n",
        "  for team_url in team_urls:\n",
        "    # Grab team name; get rid of excess words.\n",
        "    team_name = team_url.split(\"/\")[-1].replace(\"-Stats\", \"\").replace(\"-\", \" \").replace(\" Men\", \"\")\n",
        "\n",
        "    # Iterate through five 'seasons' of data.\n",
        "    for j in range(5):\n",
        "      # Grab data of each team's scores & fixtures.\n",
        "      data = requests.get(team_url)\n",
        "      matches = pd.read_html(data.text, match = \"Scores & Fixtures\")[0]\n",
        "\n",
        "      # Grab data of each team's shooting stats.\n",
        "      # The link we need has an 'a' tag with an href and must have 'all_comps/shooting/' in the link. \n",
        "      # In this case, there were multiple of the same shooting links on the same page; choose first one. \n",
        "      soup = BeautifulSoup(data.text)\n",
        "      links = [l.get(\"href\") for l in soup.find_all('a')]\n",
        "      links = [l for l in links if l and 'all_comps/shooting/' in l]\n",
        "      data = requests.get(f\"https://fbref.com{links[0]}\")\n",
        "      shooting = pd.read_html(data.text, match = \"Shooting\")[0]\n",
        "      shooting.columns = shooting.columns.droplevel()\n",
        "\n",
        "      # Merge scores & fixtures and shooting stats.\n",
        "      try:\n",
        "        team_data = matches.merge(shooting[[\"Date\", \"Sh\", \"SoT\", \"Dist\", \"PK\", \"PKatt\"]], on = \"Date\")\n",
        "      except ValueError:\n",
        "        continue\n",
        "\n",
        "      # Friendly matches don't have advanced stats, so don't include.\n",
        "      team_data = team_data[team_data[\"Comp\"] != \"Friendlies (M)\"]\n",
        "\n",
        "      # Set team name and add team data to our database.\n",
        "      team_data[\"Team\"] = team_name\n",
        "      all_matches.append(team_data)\n",
        "\n",
        "      # Previous 'season' for team; to update the for loop.\n",
        "      previous_season = soup.select(\"div.prevnext a\")[0].get(\"href\")\n",
        "      team_url = f\"https://fbref.com{previous_season}\"\n",
        "\n",
        "      # Prevent site from blocking requests.\n",
        "      time.sleep(20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k3SVM4-CbNOU",
        "outputId": "b3d26fae-a5ad-49d0-812c-204b93fe2934"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-c00ed5d83528>:44: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  team_data[\"Team\"] = team_name\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine all data and put into a csv.\n",
        "match_df = pd.concat(all_matches)\n",
        "match_df.to_csv(\"matches.csv\")"
      ],
      "metadata": {
        "id": "-kAM29sQbUBb"
      },
      "execution_count": 11,
      "outputs": []
    }
  ]
}
